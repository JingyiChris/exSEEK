shell.prefix('set -x; set -e;')
include: 'common.snakemake'

import os
import yaml
with open(data_dir + '/compare_groups.yaml', 'r') as f:
    compare_groups = yaml.load(f)

# read selected matrix preprocessing methods
feature_sets = config['evaluate_feature_sets']

preprocess_methods = {}
for count_method in to_list(config['count_method']):
    for filename in expand('{output_dir}/select_preprocess_method/{score}/{count_method}/selected_methods.txt',
        output_dir=output_dir, score=clustering_scores, count_method=count_method):
        with open(filename, 'r') as f:
            preprocess_methods[count_method] = f.readline().strip()

inputs = {'evaluate_features': []}
for count_method, preprocess_method in preprocess_methods.items():
    for compare_group in compare_groups:
        feature_set = get_known_biomarkers(compare_group)
        inputs['evaluate_features'] += expand('{output_dir}/evaluate_features/{feature_set}/{preprocess_method}.{count_method}/{compare_group}/{classifier}',
                output_dir=output_dir, count_method=count_method, preprocess_method=preprocess_method,
                classifier=config['classifier'], compare_group=compare_group, feature_set=feature_set)
inputs['summarize_evaluate_features'] = expand('{output_dir}/summary/evaluate_features/{summary_name}.txt',
    output_dir=output_dir, summary_name=['metrics.train', 'metrics.test'])

rule all:
    input:
        unpack(lambda wildcards: inputs)


rule preprocess_features:
    input:
        '{output_dir}/evaluate_features/matrix/{compare_group}/{feature_set}.txt'
    output:
        '{output_dir}/evaluate_features/preprocess_features/{compare_group}/{feature_set}.txt'
    params:
        scaler=config['scale_method']
    shell:
        '''{bin_dir}/feature_selection.py preprocess_features -i {input} --scaler {params.scaler} \
            --use-log --transpose -o {output}
        '''

"""
rule evaluate_features:
    input:
        matrix='{output_dir}/evaluate_features/preprocess_features/{compare_group}/{feature_set}.txt',
        sample_classes=data_dir+ '/sample_classes.txt'
    output:
        directory('{output_dir}/evaluate_features/result/{compare_group}/{feature_set}/{classifier}')
    params:
        n_splits=config['cross_validation_splits'],
        splitter=config['splitter'],
        positive_class=lambda wildcards: compare_groups[wildcards.compare_group][1],
        negative_class=lambda wildcards: compare_groups[wildcards.compare_group][0],
    shell:
        '''{bin_dir}/feature_selection.py evaluate -i {input.matrix} \
            --sample-classes {input.sample_classes} \
            --positive-class '{params.positive_class}' --negative-class '{params.negative_class}' \
            --method {wildcards.classifier} \
            --splitter {params.splitter} \
            --n-splits {params.n_splits} \
            --compute-sample-weight \
            -o {output}
        '''
"""

rule evaluate_features:
    input:
        matrix='{output_dir}/matrix_processing/{preprocess_method}.{count_method}.txt',
        sample_classes=data_dir+ '/sample_classes.txt',
        features=data_dir + '/known_biomarkers/{compare_group}/{feature_set}.txt'
    output:
        directory('{output_dir}/evaluate_features/{feature_set}/{preprocess_method}.{count_method}/{compare_group}/{classifier}')
    params:
        count_method=count_method_regex
    run:
        import json
        import os
        import subprocess
        from shlex import quote
        from copy import deepcopy

        command = [
            os.path.join(config['bin_dir'], 'machine_learning.py'), 'cross_validation',
            '--matrix', input.matrix,
            '--sample-classes', input.sample_classes,
            '--output-dir', output[0],
            '--transpose',
            '--positive-class', compare_groups[wildcards.compare_group][1],
            '--negative-class', compare_groups[wildcards.compare_group][0],
            '--cv-params', json.dumps(config['cv_params']),
            '--selector', 'null',
            '--features', input.features
        ]
        if config['log_transform']:
            command += ['--log-transform', '--log-transform-params', json.dumps(config['log_transform_params'])]
        if config['scaler']:
            command += ['--scaler', config['scaler'], '--scaler-params', json.dumps(config['scaler_params'].get(config['scaler'], {}))]
        #if config['grid_search']:
        #    command += ['--grid-search', '--grid-search-params', json.dumps(config['grid_search_params'])]
        if config['sample_weight']:
            command += ['--sample-weight', config['sample_weight']]
        command += ['--classifier', wildcards.classifier, 
            '--classifier-params', json.dumps(config['classifier_params'].get(wildcards.classifier, {}))]
        command = list(map(str, command))
        print(' '.join(map(quote, command)))
        subprocess.check_call(command)

rule summarize_evaluate_features:
    input:
        input_dir=inputs['evaluate_features'],
        selected_methods=lambda wildcards: expand('{output_dir}/select_preprocess_method/{score}/{count_method}/selected_methods.txt',
            output_dir=wildcards.output_dir, score=clustering_scores, count_method=config['count_method'])
    output:
        metrics_test='{output_dir}/summary/{cross_validation}/metrics.test.txt',
        metrics_train='{output_dir}/summary/{cross_validation}/metrics.train.txt'
    script:
        'scripts/summarize_cross_validation.py'