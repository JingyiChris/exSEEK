shell.prefix('set -x;')
include: '../common'

pvalues = config['pvalues']
output_dir = config['output_dir']
rna_types = config['rna_types']
rna_types_long = list(filter(lambda x: x not in ('miRNA', 'piRNA', 'rRNA'), rna_types))
rna_types_long += ['other']

piranha_path = '../piranha-exrna-1.2.1/bin'
normalize_methods = config['normalize_methods']

def get_all_inputs(wildcards):
    available_inputs = dict(
        merge_reads_by_rnatype=expand('{output_dir}/tbed_long_RNA/{sample_id}.bed.gz',
            output_dir=output_dir, sample_id=sample_ids),
        domain_count_matrix=expand('{output_dir}/count_matrix/domains_{pvalue}.txt',
            output_dir=output_dir, pvalue=pvalues)
    )
    enabled_inputs = list(available_inputs)
    inputs = []
    for key, l in available_inputs.items():
        if key in enabled_inputs:
            inputs += l
    return inputs

rule all:
    input:
        get_all_inputs

rule bam_to_bed:
    input:
        '{output_dir}/tbam/{sample_id}/{rna_type}.bam'
    output:
        '{output_dir}/tbed/{sample_id}/{rna_type}.bed.gz'
    threads: 1
    shell:
        '''bedtools bamtobed -i {input} | pigz -c -p {threads} > {output}
        '''

rule merge_reads_by_rnatype:
    input:
        lambda wildcards: expand('{output_dir}/tbed/{sample_id}/{rna_type}.bed.gz',
            output_dir=wildcards.output_dir, sample_id=wildcards.sample_id, rna_type=rna_types_long)
    output:
        '{output_dir}/tbed_long_RNA/{sample_id}.bed.gz'
    threads: 2
    shell:
        '''pigz -p {threads} -d -c {input} \
            | bedtools sort \
            | pigz -c -p {threads} > {output}'''

rule bin_coverage:
    input:
        bed='{output_dir}/tbed_long_RNA/{sample_id}.bed.gz',
        chrom_sizes=genome_dir + '/chrom_sizes/transcriptome'
    output:
        '{output_dir}/tbincov/{bin_size}/{sample_id}.bed'
    params:
        bin_size=config['bin_size']
    shell:
        '''pigz -d -c {input.bed} \
            | bin/bin_coverage /dev/stdin {input.chrom_sizes} {output} {params.bin_size}
        '''

rule call_domains:
    input:
        '{output_dir}/tbincov/{bin_size}/{sample_id}.bed'
    output:
        '{output_dir}/domains_by_sample/{bin_size}/{pvalue}/{sample_id}.bed'
    params:
        distribution=config['distribution']
    shell:
        '''{piranha_path}/Piranha -b {wildcards.bin_size} -d {params.distribution} \
            -p 0.{wildcards.pvalue} -T bin_cov \
            {input} > {output}
        '''

"""
rule call_domains_two_round:
    input:
        '{output_dir}/tbincov/{bin_size}/{sample_id}.bed'
    output:
        round1='{output_dir}/domains_by_sample/{bin_size}/{pvalue}/{sample_id}.round1.bed',
        round2='{output_dir}/domains_by_sample/{bin_size}/{pvalue}/{sample_id}.round2.bed',
        merged='{output_dir}/domains_by_sample/{bin_size}/{pvalue}/{sample_id}.bed'
    shell:
        '''{piranha_path}/Piranha -s -b {wildcards.bin_size} -d {wildcards.method} -p 0.{wildcards.pvalue} \
            -T bin_cov {input} -o {output.round1}
        bedtools intersect -v -a {input} -b {output.round1} \
            | {piranha_path}/Piranha -s -b {wildcards.bin_size} \
            -d {wildcards.method} -p 0.{wildcards.pvalue} -T bin_cov /dev/stdin -o {output.round2}
        cat {output.round1} {output.round2} \
            | bedtools sort \
            | bedtools merge -c 4,5,6,7 -o distinct,mean,distinct,mean \
            | awk 'BEGIN{{OFS="\t";FS="\t"}}{{print $1,$2,$3,"peak_"NR,$5,$6}}' > {output.merged}
        '''
"""

rule domain_recurrence:
    input:
        bed=lambda wildcards: expand('{output_dir}/domains_by_sample/{bin_size}/{pvalue}/{sample_id}.bed',
            output_dir=wildcards.output_dir,
            bin_size=wildcards.bin_size,
            pvalue=wildcards.pvalue,
            sample_id=sample_ids),
        chrom_sizes=genome_dir + '/chrom_sizes/transcriptome'
    output:
        '{output_dir}/domain_recurrence_by_strand/{bin_size}/{pvalue}.{strand}.bed'
    shell:
        '''cat {input.bed} \
            | bedtools sort \
            | bedtools genomecov -i - -strand {wildcards.strand} -g {input.chrom_sizes} -bg \
            | awk -v s={wildcards.strand} 'BEGIN{{OFS="\t";FS="\t"}}{{print $1,$2,$3,"X",$4,s}}' \
            > {output}
        '''

rule merge_domain_recurrence_by_strand:
    input:
        pos='{output_dir}/domain_recurrence_by_strand/{bin_size}/{pvalue}.+.bed',
        neg='{output_dir}/domain_recurrence_by_strand/{bin_size}/{pvalue}.-.bed'
    output:
        '{output_dir}/domains_recurrence/{bin_size}/{pvalue}.bed'
    shell:
        '''cat {input.pos} {input.neg} \
            | bedtools sort > {output}
        '''

rule filter_domain_by_recurrence:
    input:
        '{output_dir}/domains_recurrence/{bin_size}/{pvalue}.bed'
    output:
        '{output_dir}/domains/{bin_size}/{pvalue}.bed'
    params:
        cov_threshold=len(sample_ids)*config['cov_threshold']
    shell:
        '''awk -v c={params.cov_threshold} '$5 > c' {input} \
            | bedtools merge -s -c 2,3,5,6 -o collapse,collapse,collapse,collapse \
            | awk 'BEGIN{{OFS="\t";FS="\t"}} 
            {{split($4,a,/,/); split($5,b,/,/); split($6,c,/,/); split($7,d,/,/);
            cov=0.0;for(i=1;i<=length(a);i++){{cov+=c[i]*(b[i]-a[i]);}} 
            cov /= $3-$2;
            print $1,$2,$3,"peak_" NR,cov,d[1]
            }}' > {output}
        '''

rule domain_read_counts:
    input:
        bed='{output_dir}/tbed_long_RNA/{sample_id}.bed.gz',
        domains='{output_dir}/domains/{bin_size}/{pvalue}.bed'
    output:
        '{output_dir}/domain_counts/{bin_size}/{pvalue}/{sample_id}.bed'
    threads: config['threads']
    shell:
        '''pigz -d -c -p {threads} {input.bed} \
            | bedtools coverage -s -sorted -counts \
                -a {input.domains} -b - \
            | awk 'BEGIN{{OFS="\t";FS="\t"}}{{print $1,$2,$3,$4,$7,$6}}' \
                > {output}
        '''

rule summarize_domains:
    input:
        '{output_dir}/gdomain_counts/{bin_size}/{pvalue}.bed'
    output:
        directory('{output_dir}/summary/call_domains')
    shell:
        '''bin/report.py summarize_domains -i {input}
        '''

rule domain_count_matrix:
    input:
        peaks=lambda wildcards: expand('{output_dir}/domain_counts/{bin_size}/{pvalue}/{sample_id}.bed',
            output_dir=wildcards.output_dir,
            bin_size=config['bin_size'], 
            pvalue=wildcards.pvalue, 
            sample_id=sample_ids),
        transcript_table=genome_dir + '/transcript_table/all.txt',
        domains='{{output_dir}}/domains/{bin_size}/{{pvalue}}.bed'.format(bin_size=config['bin_size'])
    output:
        '{output_dir}/count_matrix/domains_{pvalue}.txt'
    params:

    run:
        import pandas as pd
        import re

        transcript_table = pd.read_table(input.transcript_table, sep='\t', dtype='str')
        transcript_table.set_index('transcript_id', drop=False, inplace=True)

        domains = pd.read_table(input.domains, sep='\t', header=None,
            names=['chrom', 'start', 'end', 'domain_id', 'score', 'strand'], dtype='str')

        pat_cov = re.compile(r'{output_dir}/domain_counts/(?P<bin_size>[^/]+)/(?P<pvalue>[^/]+)/(?P<sample_id>[^\.]+).bed'.format(output_dir=output_dir))
        mat = []
        peak_labels = None
        for filename in input.peaks:
            sample_id = pat_cov.match(filename).groupdict()['sample_id']
            df = pd.read_table(filename, header=None)
            if peak_labels is None:
                peak_labels = df.iloc[:, 3].values
            df.index = df.iloc[:, 3]
            cov = df.iloc[:, 4].copy()
            cov.name = sample_id
            mat.append(cov)
        mat = pd.concat(mat, axis=1)
        transcript_ids = domains['chrom'].values
        feature_names = transcript_table.loc[transcript_ids, 'gene_id'].values \
            + '|' + transcript_table.loc[transcript_ids, 'gene_type'].values \
            + '|' + transcript_table.loc[transcript_ids, 'gene_name'].values \
            + '|' + domains['domain_id'].values \
            + '|' + transcript_table.loc[transcript_ids, 'transcript_id'].values \
            + '|' + domains['start'].values \
            + '|' + domains['end'].values
        mat.index = feature_names
        mat.index.name = 'feature'
        mat.to_csv(output[0], sep='\t', header=True, index=True)

rule normalize_matrix:
    input:
        '{output_dir}/count_matrix/domains_{pvalue}.txt'
    output:
        '{output_dir}/normalized_matrix/{normalize_method}/domains_{pvalue}.txt'
    shell:
        '''bin/normalize.py -i {input} -m {wildcards.normalize_method} -o {output}
        '''