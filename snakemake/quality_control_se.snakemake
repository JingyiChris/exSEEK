shell.prefix('set -x;')
include: 'common.snakemake'

import os

def get_all_inputs(wildcards):
    available_inputs = dict(
        fastqc=expand('{output_dir}/fastqc/{sample_id}_fastqc.zip',
            output_dir=output_dir, sample_id=sample_ids),
        summarize_fastqc=expand('{output_dir}/summary/fastqc.txt',
            output_dir=output_dir),
        summarize_fastqc_html=expand('{output_dir}/summary/fastqc.html',
            output_dir=output_dir),
        summarize_fastqc_clean_html=expand('{output_dir}/summary/fastqc_clean.html',
            output_dir=output_dir),
        clean=expand('{output_dir}/unmapped/{sample_id}/clean.fa.gz',
            output_dir=output_dir, sample_id=sample_ids),
        summarize_cutadapt=expand('{output_dir}/summary/cutadapt.html',
            output_dir=output_dir)
    )
    enabled_inputs = list(available_inputs.keys())
    inputs = []
    for key, l in available_inputs.items():
        if key in enabled_inputs:
            inputs += l
    return inputs

rule all:
    input:
        get_all_inputs


rule cutadapt_se:
    input:
        auto_gzip_input(data_dir + '/fastq/{sample_id}.fastq')
    output:
        trimmed='{output_dir}/cutadapt/{sample_id}.fastq.gz'
    params:
        min_read_length=config['min_read_length'],
        min_base_quality=config['min_base_quality'],
        trim_5p=lambda wildcards: '-u {}'.format(config['trim_5p']) if config['trim_5p'] > 0 else '',
        trim_3p=lambda wildcards: '-u -{}'.format(config['trim_3p']) if config['trim_3p'] > 0 else '',
        adaptor=lambda wildcards: '-a {}'.format(config['adaptor']) if len(config['adaptor']) > 0 else '',
        adaptor_5p=lambda wildcards: '-g {}'.format(config['adaptor_5p']) if len(config['adaptor_5p']) > 0 else '',
        umi_length=config['umi_length']
    log:
        '{output_dir}/log/cutadapt/{sample_id}'
    threads: 2
    run:
        if config['umi_tags']:
            shell('''{bin_dir}/auto_uncompress {input} \
            | {bin_dir}/cut_umi_tags.py -u {params.umi_length} \
            | cutadapt {params.adaptor} {params.trim_3p} \
                -m {params.min_read_length} --trim-n -q {params.min_base_quality} - \
                -o >(gzip -c -p {threads} > {output.trimmed}) > {log} 2>&1
            ''')
        else:
            shell('''cutadapt {params.adaptor} {params.adaptor_5p} {params.trim_5p} {params.trim_3p} \
            -m {params.min_read_length} --trim-n -q {params.min_base_quality} \
            -o >(gzip -c -p {threads} > {output.trimmed}) {input} > {log} 2>&1
            ''')

rule summarize_cutadapt_se:
    input:
        lambda wildcards: expand('{output_dir}/log/cutadapt/{sample_id}',
            output_dir=wildcards.output_dir, sample_id=sample_ids)
    output:
        '{output_dir}/summary/cutadapt.txt'
    run:
        import pandas as pd
        
        def parse_number(s):
            return int(''.join(s.split(',')))

        columns = ['sample_id', 'total_reads', 'reads_with_adapters', 'reads_too_short', 'reads_kept',
            'total_bp', 'bp_quality_trimmed', 'bp_kept']
        summary = []
        for filename in input:
            sample_id = os.path.basename(filename)
            record = {'sample_id': sample_id}
            with open(filename, 'r') as fin:
                for line in fin:
                    line = line.strip()
                    if line.startswith('Total reads processed:'):
                        record['total_reads'] = parse_number(line.split()[-1])
                    elif line.startswith('Reads with adapters:'):
                        record['reads_with_adapters'] = parse_number(line.split()[-2])
                    elif line.startswith('Reads that were too short:'):
                        record['reads_too_short'] = parse_number(line.split()[-2])
                    elif line.startswith('Reads written (passing filters):'):
                        record['reads_kept'] = parse_number(line.split()[-2])
                    elif line.startswith('Total basepairs processed:'):
                        record['total_bp'] = parse_number(line.split()[-2])
                    elif line.startswith('Quality-trimmed:'):
                        record['bp_quality_trimmed'] = parse_number(line.split()[-3])
                    elif line.startswith('Total written (filtered):'):
                        record['bp_kept'] = parse_number(line.split()[-3])
            summary.append(record)
        summary = pd.DataFrame.from_records(summary)
        summary = summary.reindex(columns=columns)
        summary.to_csv(output[0], sep='\t', na_rep='NA', index=False, header=True)

rule summarize_cutadapt_jupyter_se:
    input:
        summary='{output_dir}/summary/cutadapt.txt',
        jupyter=root_dir + '/templates/summarize_cutadapt_se.ipynb'
    output:
        jupyter='{output_dir}/summary/cutadapt.ipynb',
        html='{output_dir}/summary/cutadapt.html'
    run:
        shell(nbconvert_command)

rule fastq_to_fasta_se:
    input:
        '{output_dir}/cutadapt/{sample_id}.fastq.gz'
    output:
        '{output_dir}/unmapped/{sample_id}/clean.fa.gz'
    threads:
        config['threads_compress']
    shell:
        '''pigz -d -c -p {threads} {input} | fastq_to_fasta -r | pigz -p {threads} -c > {output}
        '''

rule fastqc:
    input:
        auto_gzip_input(data_dir + '/fastq/{sample_id}.fastq')
    output:
        html='{output_dir}/fastqc/{sample_id}_fastqc.html',
        zip='{output_dir}/fastqc/{sample_id}_fastqc.zip'
    params:
        output_prefix='{output_dir}/fastqc/',
        temp_dir=config['temp_dir']
    log:
        '{output_dir}/log/fastqc/{sample_id}'
    shell:
        '''fastqc -q -o {params.output_prefix} -d {params.temp_dir} {input} > {log} 2>&1
        '''

rule fastqc_clean:
    input:
        '{output_dir}/cutadapt/{sample_id}.fastq.gz'
    output:
        html='{output_dir}/fastqc_clean/{sample_id}_fastqc.html',
        zip='{output_dir}/fastqc_clean/{sample_id}_fastqc.zip'
    params:
        output_prefix='{output_dir}/fastqc_clean/',
        temp_dir=config['temp_dir']
    log:
        '{output_dir}/log/fastqc_clean/{sample_id}'
    shell:
        '''fastqc -q -o {params.output_prefix} -d {params.temp_dir} {input} > {log} 2>&1
        '''

def parse_fastqc_data(fp):
    section = None
    qc_status = OrderedDict()
    basic_statistics = OrderedDict()
    for line in fp:
        line = str(line, encoding='utf-8')
        line = line.strip()
        if line.startswith('>>'):
            if line == '>>END_MODULE':
                continue
            section, status = line[2:].split('\t')
            qc_status[section] = status
        else:
            if section == 'Basic Statistics':
                key, val = line.split('\t')
                basic_statistics[key] = val
    for key, val in qc_status.items():
        basic_statistics[key] = val
    return basic_statistics

rule summarize_fastqc:
    input:
        zip=lambda wildcards: expand('{output_dir}/{fastqc_step}/{sample_id}_fastqc.zip',
            output_dir=wildcards.output_dir, fastqc_step=wildcards.fastqc_step, sample_id=sample_ids)
    output:
        '{output_dir}/summary/{fastqc_step}.txt'
    wildcard_constraints:
        fastqc_step='fastqc.*'
    run:
        import pandas as pd
        from zipfile import ZipFile
        import os
        from collections import OrderedDict

        summary = OrderedDict()
        columns = None
        for filename in input.zip:
            sample_id = os.path.splitext(os.path.basename(filename))[0][:-7]
            with ZipFile(filename, 'r') as zf:
                with zf.open(sample_id + '_fastqc/fastqc_data.txt', 'r') as f:
                    summary[sample_id] = parse_fastqc_data(f)
                    if columns is None:
                        columns = list(summary[sample_id].keys())
        summary = pd.DataFrame.from_records(summary)
        summary = summary.T
        summary = summary.reindex(columns=columns)
        summary.index.name = 'sample_id'
        summary.to_csv(output[0], sep='\t', index=True, header=True) 

rule summarize_fastqc_jupyter:
    input:
        fastqc='{output_dir}/summary/{fastqc_step}.txt',
        jupyter=root_dir + '/templates/fastqc.ipynb'
    output:
        jupyter='{output_dir}/summary/{fastqc_step}.ipynb',
        html='{output_dir}/summary/{fastqc_step}.html'
    wildcard_constraints:
        fastqc_step='fastqc.*'
    run:
        shell(nbconvert_command)


rule prinseq_clean_se:
    input:
        fastq='{output_dir}/cutadapt/{sample_id}.fastq.gz'
    output:
        graph_data='{output_dir}/prinseq_clean/{sample_id}.gd'
    shell:
        '''perl {tools_dir}/prinseq/prinseq-lite.pl -verbose -fastq <(zcat {input.fastq}) \
            -ns_max_n 0 -graph_data {output.graph_data} -out_good null -out_bad null
        '''

rule prinseq_graph_clean_se:
    input:
        '{output_dir}/prinseq_clean/{sample_id}.gd'
    output:
        '{output_dir}/prinseq_clean/{sample_id}.html'
    shell:
        '''perl {tools_dir}/prinseq/prinseq-graphs.pl -i {input} -html_all -o {output}
        '''