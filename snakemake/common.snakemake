shell.prefix('set -x;')
import os
import yaml
import re

with open('snakemake/default_config.yaml', 'r') as f:
    default_config = yaml.load(f)

default_config.update(config)
config = default_config

def require_variable(variable, condition=None):
    value = config.get(variable)
    if value is None:
        raise ValueError('configuration variable "{}" is required'.format(variable))
    if (condition == 'input_dir') and (not os.path.isdir(value)):
        raise ValueError('cannot find input directory {}: {}'.format(variable, value))
    elif (condition == 'input_file') and (not os.path.isfile(value)):
        raise ValueError('cannot find input file {}: {}'.format(variable, value))
    return value

dataset = config['dataset']
root_dir = require_variable('root_dir', 'input_dir')
data_dir = require_variable('data_dir', 'input_dir')
genome_dir = require_variable('genome_dir', 'input_dir')
bin_dir = require_variable('bin_dir', 'input_dir')
output_dir = require_variable('output_dir')
rna_types = require_variable('rna_types')
tools_dir = require_variable('tools_dir')
temp_dir = require_variable('temp_dir')

# read sample ids from file
with open(os.path.join(data_dir, 'sample_ids.txt'), 'r') as f:
    sample_ids = f.read().split()
for sample_id in sample_ids:
    if '.' in sample_id:
        raise ValueError('"." is not allowed in sample ID: {}'.format(sample_id))

def get_preprocess_methods():
    preprocess_methods = []
    for batch_removal_method in config['batch_removal_methods']:
        if batch_removal_method == 'Combat':
            template = 'filter.{imputation_method}.Norm_{normalization_method}.Batch_{batch_removal_method}_{batch_index}'
            preprocess_methods += expand(template,
                output_dir=output_dir,
                imputation_method=config['imputation_methods'],
                normalization_method=config['normalization_methods'],
                batch_removal_method=batch_removal_method,
                batch_index=config['batch_indices'])
        else:
            template = 'filter.{imputation_method}.Norm_{normalization_method}.Batch_{batch_removal_method}'
            preprocess_methods += expand(template,
                output_dir=output_dir,
                imputation_method=config['imputation_methods'],
                normalization_method=config['normalization_methods'],
                batch_removal_method=batch_removal_method)
    return preprocess_methods

def auto_gzip_input(template):
    def get_filename(wildcards):
        gzip_names = expand(template + '.gz', **wildcards)
        if all(os.path.exists(f) for f in gzip_names):
            return gzip_names
        original_names = expand(template, **wildcards)
        #if all(os.path.exists(f) for f in original_names):
        return original_names
        
    return get_filename

def parse_fastqc_data(fp):
    section = None
    qc_status = OrderedDict()
    basic_statistics = OrderedDict()
    for line in fp:
        line = str(line, encoding='utf-8')
        line = line.strip()
        if line.startswith('>>'):
            if line == '>>END_MODULE':
                continue
            section, status = line[2:].split('\t')
            qc_status[section] = status
        else:
            if section == 'Basic Statistics':
                key, val = line.split('\t')
                basic_statistics[key] = val
    for key, val in qc_status.items():
        basic_statistics[key] = val
    return basic_statistics
    
def get_input_matrix(wildcards):
    # Use RPM for small RNA
    if config['small_rna']:
        return '{output_dir}/matrix_processing/{preprocess_method}.{count_method}.txt'.format(**wildcards)
    # Use RPKM for long RNA
    else:
        return '{output_dir}/rpkm/{preprocess_method}.{count_method}.txt'.format(**wildcards)
    
def get_known_biomarkers():
    if os.path.isdir(os.path.join(data_dir, 'known_biomarkers')):
        for compare_group in os.listdir(os.path.join(data_dir, 'known_biomarkers')):
            for feature_file in os.listdir(os.path.join(data_dir, 'known_biomarkers', compare_group)):
                feature_set = os.path.splitext(feature_file)[0]
                yield (compare_group, feature_set)

def to_list(obj):
    if isinstance(obj, list):
        return obj
    if isinstance(obj, str) or isinstance(obj, bytes):
        return [obj]
    return list(obj)

# template for nbconvert
nbconvert_command = '''cp {input.jupyter} {output.jupyter}
jupyter nbconvert --execute --to html \
    --HTMLExporter.exclude_code_cell=False \
    --HTMLExporter.exclude_input_prompt=True \
    --HTMLExporter.exclude_output_prompt=True \
    {output.jupyter}
'''

# export singularity wrappers
use_singularity = config.get('use_singularity')
if use_singularity:
    os.environ['PATH'] = config['singularity']['wrapper_dir'] + ':' + os.environ['PATH']

rule make_temp:
    output:
        temp_dir
    shell:
        'mkdir -p {output}'

count_method_regex = '(featurecounts)|(htseq)|(domains_combined)|(domains_long)|(transcript)|(transcript_small)|(transcript_long)|(transcript_long_bg)|(mirna_and_domains)|(mirna_and_long_bg)|(transcript_mirna)'
imputation_method_regex = '(scimpute_count)|(viper_count)|(null)'
normalization_method_regex = '(SCnorm)|(TMM)|(RLE)|(CPM)|(CPM_top)|(CPM_rm)|(CPM_refer)|(null)'
batch_removal_method_with_batchinfo_regex = '(Combat)'
batch_removal_method_without_batchinfo_regex = '(RUV)|(null)'